{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Flux\n",
    "using Random, Distributions, LinearAlgebra, DataFrames\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Regression problems pop up whenever we want to predict a numerical value. Common examples include predicting prices\n",
    "\n",
    "As a running example, suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years). To develop a model for predicting house prices\n",
    "\n",
    "$$\n",
    "price = w_{area} * area + w_{age} * age + b\n",
    "$$\n",
    "\n",
    "Where w is weight which determine the influence of each variable and b the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function tst(n)\n",
    "    a = ones(1, n)\n",
    "    b = ones(1, n)\n",
    "\n",
    "    for i in range(1, n)\n",
    "        return a[i] + b[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time tst(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting all features into a vector X and all weights into a vector W, we can express our model compactly via the dot product between X and W\n",
    "$$\n",
    "\\hat{y} = w * x + b\n",
    "$$\n",
    "\n",
    "### Loss Function\n",
    "Before we can go about searching for the best parameters (or model parameters) w and b, we will need two more things: \n",
    "- a quality measure for some given model\n",
    "- a procedure for updating the model to improve its quality.\n",
    "$$\n",
    "l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent to reduce Loss Function to find local minimum\n",
    "The most common use of gradient descent consists of taking the derivative of the loss function. \n",
    "(In practice, this can be extremely slow: we must pass over the entire dataset before making a single update)\n",
    "\n",
    " Even worse, if there is a lot of redundancy in the training data, the benefit of a full update is even lower.\n",
    "$$\n",
    "(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}_t} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).\n",
    "$$\n",
    "\n",
    "We sample a minibatch Beta consisting of a fixed number Beta{t} of training examples. We then compute the derivative (gradient) of the average loss on the minibatch with respect to the model parameters. Finally, we multiply the gradient by a predetermined small positive value L, called the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function normal(x, mu, sigma)\n",
    "    p = 1 / sqrt(2 * π * sigma^2)\n",
    "    return p * exp(-0.5 * (x - mu)^2 / sigma^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "Linear regression was invented at the turn of the 19th century. While it has long been debated whether Gauss or Legendre first thought up the idea, it was Gauss who also discovered the normal distribution (also called the Gaussian). It turns out that the normal distribution and linear regression with squared loss share a deeper connection than common parentage.\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can now write out the likelihood of seeing a particular X for a given Y\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "$$ given \\ a \\ model \\ \\ \\ \\hat{\\mathbf{w}}^\\top \\mathbf{x} + \\hat{b}\\ $$\n",
    "\n",
    "we can now make predictions for a new example, e.g., to predict the sales price of a previously unseen house given its area \n",
    "and age \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = collect(-10.0:0.1:10.0)\n",
    "# μ is the mean\n",
    "# σ is the standard deviaton\n",
    "# we choose some values such as μ = 0 and σ = 1 and visualize the normal distribution\n",
    "\n",
    "y = normal.(A, 0.0, 3.0)\n",
    "\n",
    "plot(A, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
